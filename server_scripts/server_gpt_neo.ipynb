{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bc14d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904510f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# GPT-Neo Server Code (Kaggle Notebook 2)\n",
    "from fastapi import FastAPI, HTTPException, Request, Response\n",
    "import uvicorn\n",
    "from pyngrok import ngrok, conf\n",
    "import threading\n",
    "import time\n",
    "import torch\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import msgpack\n",
    "\n",
    "# Ngrok config\n",
    "NGROK_AUTHTOKEN = \"YOUR_NGROK_AUTHTOKEN\"\n",
    "conf.get_default().auth_token = NGROK_AUTHTOKEN\n",
    "\n",
    "# Load GPT-Neo model\n",
    "print(\"Loading GPT-Neo...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", torch_dtype=torch.float16)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"GPT-Neo model loaded on {device}\")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"GPT-Neo Server is running!\", \"model\": \"gpt-neo\", \"version\": \"v2.0\"}\n",
    "\n",
    "@app.post(\"/inference\")\n",
    "async def inference(request: Request):\n",
    "    \"\"\"Accept msgpack data directly from request body\"\"\"\n",
    "    try:\n",
    "        body = await request.body()\n",
    "        print(f\"Received body length: {len(body)}\")\n",
    "        \n",
    "        request_data = msgpack.unpackb(body, raw=False)\n",
    "        text = request_data['text']\n",
    "        do_generate = request_data.get('do_generate', False)\n",
    "        \n",
    "        print(f\"Processing text: {text[:50]}...\")\n",
    "        \n",
    "        # Tokenize with shorter max_length to avoid OOM\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = inputs['input_ids'][..., 1:].contiguous()\n",
    "            \n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "            losses = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            losses = losses.view(shift_labels.shape)\n",
    "            \n",
    "            token_losses = losses[0].cpu().numpy().tolist()\n",
    "            begin_word_idx = 1\n",
    "            ll_tokens = [-loss for loss in token_losses]\n",
    "        \n",
    "        response_tuple = (token_losses, begin_word_idx, ll_tokens)\n",
    "        packed_response = msgpack.packb(response_tuple)\n",
    "        \n",
    "        print(f\"Response prepared successfully\")\n",
    "        \n",
    "        return Response(\n",
    "            content=packed_response, \n",
    "            media_type=\"application/octet-stream\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "def run_app():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Start server\n",
    "ngrok.kill()\n",
    "server_thread = threading.Thread(target=run_app)\n",
    "server_thread.start()\n",
    "time.sleep(5)\n",
    "\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"GPT-Neo Server URL: {public_url}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "while True:\n",
    "    time.sleep(60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
